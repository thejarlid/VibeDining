# VibeDining - Intelligent Restaurant Recommendation System

## Overview

VibeDining is a comprehensive restaurant recommendation system that combines data scraping, intelligent indexing, and agentic search to provide personalized dining recommendations. The system processes user-saved restaurant lists and uses multi-modal search techniques to understand both explicit constraints and qualitative preferences.

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           VibeDining System                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   SCRAPING  â”‚â”€â”€â”€â–¶â”‚   INDEXING  â”‚â”€â”€â”€â–¶â”‚ AGENTIC AI  â”‚                 â”‚
â”‚  â”‚   PIPELINE  â”‚    â”‚   SYSTEM    â”‚    â”‚ RECOMMENDER â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                         â”‚
â”‚  â€¢ Google Takeout    â€¢ SQLite +       â€¢ LangGraph Agent               â”‚
â”‚  â€¢ Web Scraping      Vector Store     â€¢ Multi-tool Search             â”‚
â”‚  â€¢ Place Data        â€¢ Neighborhoods  â€¢ Quality Validation            â”‚
â”‚  â€¢ API Integration   â€¢ Semantic Index â€¢ Natural Language             â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Data Flow

```mermaid
graph TD
    A[Google Takeout CSV] --> B[Scraping Pipeline]
    B --> C[Place Data Extraction]
    C --> D[Dual Storage System]
    
    D --> E[SQLite Database]
    D --> F[Vector Store]
    
    E --> G[Structured Queries]
    F --> H[Semantic Search]
    
    G --> I[Agentic Recommender]
    H --> I
    
    I --> J[Quality Validation]
    J --> K[Personalized Recommendations]
    
    style A fill:#e1f5fe
    style D fill:#f3e5f5
    style I fill:#e8f5e8
    style K fill:#fff3e0
```

---

## ğŸ•·ï¸ Scraping Pipeline

### Purpose
Extract comprehensive restaurant data from Google Takeout saved lists using cost-optimized web scraping combined with minimal API calls.

### Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚          ScrapingPipeline               â”‚
                    â”‚                                         â”‚
                    â”‚  â€¢ Orchestrates entire flow            â”‚
                    â”‚  â€¢ Manages checkpointing                â”‚
                    â”‚  â€¢ Controls concurrency (semaphore)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         Processing Flow                 â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚CSVProcessor â”‚    â”‚ PlaceScraperâ”‚    â”‚CheckpointMgrâ”‚    â”‚ PlaceStore  â”‚
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â”‚â€¢ parse_csv()â”‚â”€â”€â”€â–¶â”‚â€¢ scrape()   â”‚â”€â”€â”€â–¶â”‚â€¢ save()     â”‚â”€â”€â”€â–¶â”‚â€¢ persist()  â”‚
â”‚             â”‚    â”‚â€¢ get_id()   â”‚    â”‚â€¢ load()     â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚â€¢ scrape_dataâ”‚    â”‚â€¢ is_cached()â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚â€¢ api_call() â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              
                   â”‚â€¢ browser    â”‚                                
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               
```

### Key Components

#### 1. **ScrapingPipeline** (Orchestrator)
- **Purpose**: Central coordinator managing the entire scraping process
- **Responsibilities**:
  - Load CSV files and coordinate processing
  - Control concurrency with semaphore limits
  - Handle checkpointing strategy
  - Error handling and retry logic
  - Final data persistence

#### 2. **PlaceScraper** (Core Engine)
- **Purpose**: Extract place data using web scraping + minimal API calls
- **Process**:
  ```python
  async def scrape_place(self, place: CSVPlaceData) -> Place:
      # 1. Get place_id from CID (web scraping)
      place_id = await self._get_place_id(page, place)
      
      # 2. Run in parallel:
      api_task = asyncio.create_task(self._get_api_data(place_id))
      scraping_task = asyncio.create_task(self._scrape_detailed_data(page, place_id))
      
      # 3. Combine results
      api_data, scraped_data = await asyncio.gather(api_task, scraping_task)
      return self._build_place(place, place_id, api_data, scraped_data)
  ```

#### 3. **CheckpointManager** (Persistence)
- **Purpose**: Handle incremental processing and crash recovery
- **Features**:
  - Load existing checkpoint data on startup
  - Save processed places immediately to CSV
  - Staleness detection and re-processing

### Data Models

```python
@dataclass
class CSVPlaceData:
    name: str
    url: str

@dataclass
class Place:
    # Core identifiers
    name: str
    place_id: str
    url: str
    
    # API fields (minimal/free)
    formatted_address: Optional[str]
    coordinates: Optional[tuple[float, float]]
    
    # Scraped rich data
    attributes: List[str]  # Atmospheric attributes
    reviews: List[dict]    # Customer reviews
    rating: Optional[float]
    price_level: Optional[str]
    category: Optional[str]
    
    # Metadata
    last_scraped: str
```

---

## ğŸ—„ï¸ Indexing System

### Purpose
Create a dual-storage system optimized for both structured queries and semantic search.

### Database Schema

```sql
-- Core restaurant data
CREATE TABLE Places (
    id TEXT PRIMARY KEY,
    name TEXT,
    rating REAL,
    price_level TEXT,
    category TEXT,
    formatted_address TEXT,
    description TEXT,
    business_status TEXT,
    -- ... other fields
);

-- Neighborhood/location data
CREATE TABLE Localities (
    id TEXT PRIMARY KEY,
    name TEXT,
    full_name TEXT,
    latitude REAL,
    longitude REAL,
    type TEXT CHECK(type IN ('neighborhood', 'city'))
);

-- Many-to-many relationship
CREATE TABLE PlaceLocalities (
    place_id TEXT REFERENCES Places(id),
    locality_id TEXT REFERENCES Localities(id),
    PRIMARY KEY(place_id, locality_id)
);
```

### Vector Store Structure

**ChromaDB Collections:**
- **Document Types**: `description`, `atmosphere`, `food_drink`, `special_features`
- **Embeddings**: OpenAI `text-embedding-3-small`
- **Reranking**: Cross-encoder model for relevance refinement
- **Metadata**: Place ID, name, document type for cross-referencing

### LLM-Enhanced Data Processing

```python
def _summarize_place_with_llm(self, place: Place):
    # Convert raw scraped data into semantic documents
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user", 
            "content": f"Summarize venue atmosphere and features for: {place.data}"
        }]
    )
    return {
        'atmosphere': "Cozy, intimate setting perfect for dates...",
        'food_drink': "Specializes in craft cocktails and small plates...", 
        'special_features': "Live jazz music on weekends, outdoor seating..."
    }
```

---

## ğŸ¤– Agentic Recommender

### Architecture Overview

The agentic system uses **LangGraph** to create an intelligent recommendation agent that can dynamically choose and combine multiple search strategies.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 AgenticRecommender                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ GUARDRAIL   â”‚â”€â”€â”€â–¶â”‚ MULTI-TOOL  â”‚â”€â”€â”€â–¶â”‚  RESPONSE   â”‚ â”‚
â”‚  â”‚ VALIDATION  â”‚    â”‚   AGENT     â”‚    â”‚ GENERATION  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚  â€¢ Query Intent     â€¢ Vector Search    â€¢ Quality Check â”‚
â”‚  â€¢ Topic Relevance  â€¢ SQL Filtering    â€¢ Data Limits  â”‚
â”‚  â€¢ Safety Check     â€¢ Location Valid   â€¢ Honest Reply â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

#### 1. **Guardrail System**
```python
class GuardrailResult(BaseModel):
    allowed: bool = Field(description="Whether query is restaurant-related")
    reason: str = Field(description="Reasoning for decision")

def _guardrail(self, state: State):
    # LLM-based intent classification
    result = self._guardrail_llm.invoke([
        SystemMessage(content="Classify if query is restaurant-related..."),
        HumanMessage(content=state["input"])
    ])
    return {"guardrail_result": result}
```

#### 2. **Multi-Tool Agent**
The agent has access to multiple specialized tools:

```python
class RestaurantSearchTools:
    def vector_search(self, query: str, n_results: int = 20):
        """Semantic similarity search for qualitative features"""
        
    def sql_search(self, constraints: str):
        """SQL queries for specific constraints (location, price, rating)"""
        
    def validate_location_match(self, place_id: str, target_location: str):
        """Verify if results actually match location constraints"""
        
    def get_restaurant_details(self, place_id: str):
        """Get comprehensive details for specific restaurants"""
```

#### 3. **Tool Usage Strategy**

**Vector Search**: Used for qualitative queries
- "cozy atmosphere", "romantic dinner", "good for work"
- Semantic understanding of vibe and ambiance
- Reviews and description analysis

**SQL Search**: Used for specific constraints  
- Neighborhoods: "in East Village", "Williamsburg area"
- Price ranges: "cheap", "$$ level", "under $20"
- Ratings: "highly rated", "4+ stars"
- Cuisine: "Italian", "sushi", "coffee shops"

**Location Validation**: Quality control
- Cross-reference vector results with location constraints
- Filter out results that don't match specified neighborhoods
- Use join table to verify place-locality relationships

### Intelligence & Quality Control

#### Smart Query Processing
```python
# Example system prompt excerpt:
"""
TOOL USAGE STRATEGY:
1. **vector_search**: Use for qualitative queries (atmosphere, vibe)
2. **sql_search**: Use for specific constraints (location, price, cuisine)
3. **validate_location_match**: Verify results match location requirements
4. **get_restaurant_details**: Get full info for promising results

RECOMMENDED APPROACH:
- For queries with LOCATION + VIBE: Use BOTH vector_search AND sql_search
- For queries with specific constraints: Start with sql_search
- ALWAYS cross-reference when location is mentioned
- VALIDATE each result against user constraints
"""
```

#### Quality Validation
```python
# Built-in quality control:
QUALITY_CONTROL = {
    "location_mismatch": "EXCLUDE if doesn't match neighborhood",
    "category_mismatch": "EXCLUDE if wrong type (restaurant vs coffee shop)",
    "insufficient_data": "BE HONEST about limited saved lists",
    "accuracy_over_quantity": "Better to admit data gaps than give bad results"
}
```

#### Honest Data Limitation Handling
```python
# When data is limited:
response_format = """
I found {X} places in your saved lists that match your criteria, but the selection is limited. 

Here's what I found:
[list the few good matches]

Your saved lists don't seem to have many {coffee shops/restaurants} in {location}. 
In the future, I could search the web to find additional options that match your preferences.
"""
```

### State Management

**LangGraph State Extension:**
```python
class State(AgentState):  # Extends LangGraph's AgentState
    # Additional fields for our use case
    input: str
    output: Optional[str] 
    guardrail_result: Optional[GuardrailResult]
    # Inherits: messages (for ReAct agent compatibility)
```

---

## ğŸ”„ Complete System Flow

### 1. **Data Ingestion** (Scraping Pipeline)
```
Google Takeout CSV â†’ Parse Places â†’ Concurrent Scraping â†’ Rich Place Data
```

### 2. **Data Processing** (Indexing System)  
```
Place Data â†’ LLM Summarization â†’ Dual Storage (SQL + Vector) â†’ Searchable Index
```

### 3. **User Query** (Agentic Recommender)
```
User Input â†’ Guardrail â†’ Multi-Tool Search â†’ Quality Validation â†’ Response
```

### Example Query Flow

**User**: *"Find me a cozy coffee shop in Williamsburg where I can work"*

1. **Guardrail**: âœ… Restaurant-related query approved
2. **Agent Analysis**: Location (Williamsburg) + Vibe (cozy, work-friendly) + Category (coffee shop)
3. **Tool Execution**:
   - `sql_search("coffee shops in Williamsburg")` â†’ Find Williamsburg coffee shops
   - `vector_search("cozy coffee shop good for work")` â†’ Find work-friendly atmospheres
   - `validate_location_match(place_ids, "Williamsburg")` â†’ Verify locations
4. **Quality Control**: Filter results that don't match constraints
5. **Response**: Provide 2-3 validated recommendations or honest "limited data" response

---

## ğŸš€ Key Benefits

### **Cost Optimization**
- Scraping-first approach minimizes API costs
- LLM processing creates rich semantic data from free scraped content
- Efficient vector storage with smart document chunking

### **Intelligent Search**
- Multi-modal approach: structured + semantic + validation
- Agent decides optimal tool combination per query
- Quality-first results with honest limitation acknowledgment

### **Scalable Architecture**
- Modular design allows independent component scaling
- Async processing for high throughput
- Clean separation between scraping, indexing, and recommendation

### **User Experience**
- Natural language queries ("cozy", "romantic", "good for work")
- Location-aware recommendations
- Transparent about data limitations
- Future-ready for web search enrichment

---

## ğŸ› ï¸ Setup & Usage

### Dependencies
```bash
pip install -r requirements.txt
```

### Configuration
```python
# Required environment variables
OPENAI_API_KEY=your_openai_key
GOOGLE_MAPS_API_KEY=your_google_maps_key  # For basic fields only
```

### Usage Examples

#### Scraping
```python
from src.scraping_pipeline import ScrapingPipeline

pipeline = ScrapingPipeline()
await pipeline.process_csv_file("my_saved_places.csv")
```

#### Indexing  
```python
from src.indexer import Indexer

indexer = Indexer(db_path='places.db', chroma_path='places_vector_db')
indexer.index_csv('scraped_places.csv')
```

#### Recommendations
```python
from src.simple_agent import AgenticRecommender

agent = AgenticRecommender(debug=True)
response = agent.query("Find me a romantic Italian restaurant in East Village")
print(response)
```

---

## ğŸ”® Future Enhancements

### **Web Search Integration**
- Automatic web search when saved lists have insufficient data
- Real-time hours, menu updates, recent reviews
- Dynamic data enrichment

### **Advanced Personalization**
- User preference learning
- Historical recommendation feedback
- Collaborative filtering with other users

### **Enhanced Data Sources**
- Multiple platform integration (Yelp, OpenTable, etc.)
- Social media sentiment analysis
- Real-time event and crowd data

---

## ğŸ“ Project Structure

```
vibedining/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraping_pipeline.py     # Data extraction pipeline
â”‚   â”œâ”€â”€ indexer.py              # Dual storage system
â”‚   â”œâ”€â”€ simple_agent.py         # Agentic recommender
â”‚   â”œâ”€â”€ query.py               # Alternative graph-based agent
â”‚   â””â”€â”€ model.py               # Data models
â”œâ”€â”€ places.db                  # SQLite database
â”œâ”€â”€ places_vector_db/          # ChromaDB vector store
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md                 # This file
```

---

## ğŸ¯ Design Philosophy

**Data-Driven Intelligence**: Leverage user's actual saved places rather than generic recommendations

**Quality Over Quantity**: Better to provide fewer accurate results than many irrelevant ones

**Transparency**: Be honest about data limitations and system capabilities

**Cost Efficiency**: Maximize value from free/low-cost data sources before expensive API calls

**User Intent Understanding**: Go beyond keyword matching to understand qualitative preferences

**Modular Architecture**: Each component can be improved or replaced independently

This architecture creates a foundation for highly personalized, intelligent restaurant recommendations that understand both explicit constraints and subtle preferences while maintaining cost efficiency and result quality.